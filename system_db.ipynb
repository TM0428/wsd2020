{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "system_db.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TM0428/wsd2020/blob/main/system_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23pRMcRfWQ5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bab0e9e-34d0-4670-8eec-a314af691a16"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def dirsName():\n",
        "    return \"data/\"\n",
        "\n",
        "def fileName(name: str, index: int, extension: str=\"html\")->str:\n",
        "    return dirsName() + name + \"/\" + str(index) + \".\" + extension\n",
        "\n",
        "def urlName(article_url: str, article_index:int)->str:\n",
        "    return article_url + str(article_index)\n",
        "\n",
        "def main():\n",
        "    contents = [ # name, domain, article_urlは手動でせざるを得ないか、article_indexを自動で取ってこられるようにしたい\n",
        "        {\n",
        "            \"name\": \"sejuku\",\n",
        "            \"domain\":\"https://www.sejuku.net/\",\n",
        "            \"article_url\":\"https://www.sejuku.net/blog/\",\n",
        "            \"article_index\": [33294, 3766, 105909\n",
        "            ] # 記事番号は https://www.sejuku.net/blog/curriculums-python から適当にとってきた。実際は https://www.sejuku.net/blog/archive/page/* から適当に取ってくるといいかもしれない\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"techacademy\",\n",
        "            \"domain\":\"https://techacademy.jp/\",\n",
        "            \"article_url\":\"https://techacademy.jp/magazine/\",\n",
        "            \"article_index\": [15508, 15636, 15571] # 記事番号は https://techacademy.jp/magazine/category/programming/python/page/44 から適当にとってきた\n",
        "        }\n",
        "    ]\n",
        "    for content in contents:\n",
        "        print(content[\"name\"])\n",
        "        for index in content[\"article_index\"]:\n",
        "            filename = fileName(content[\"name\"], index) # ローカル保存用のファイル名\n",
        "            print(filename)\n",
        "            print(os.path.exists(filename))\n",
        "            if not os.path.exists(filename): # 1回getしたらローカルに保存して、複数回getしない\n",
        "                urlname = urlName(content['article_url'], index) # get用のurl名\n",
        "                print(urlname)\n",
        "                html = requests.get(urlname) # getしたhtml(bytes)\n",
        "                html_code = html.content.decode('utf-8') #getしたhtml(string)\n",
        "                os.makedirs(dirsName(), exist_ok=True) # ディレクトリを作成\n",
        "                os.makedirs(dirsName() + content[\"name\"] + \"/\", exist_ok=True) # ディレクトリを作成\n",
        "                with open(filename, mode='w', encoding='utf-8') as f:\n",
        "                    f.write(html_code) # ローカルに保存\n",
        "                time.sleep(3) # 適度なインターバル\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sejuku\n",
            "data/sejuku/33294.html\n",
            "False\n",
            "https://www.sejuku.net/blog/33294\n",
            "data/sejuku/3766.html\n",
            "False\n",
            "https://www.sejuku.net/blog/3766\n",
            "data/sejuku/105909.html\n",
            "False\n",
            "https://www.sejuku.net/blog/105909\n",
            "techacademy\n",
            "data/techacademy/15508.html\n",
            "False\n",
            "https://techacademy.jp/magazine/15508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTnV42vWW-6I"
      },
      "source": [
        "# html to text\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import NavigableString\n",
        "import re\n",
        "\n",
        "# text_setを生成する\n",
        "# text_setは各葉ノードのテキストと木構造を表す\n",
        "def soup2textset(tag, text_set, tag_string):\n",
        "    for content in tag.contents:\n",
        "        if isinstance(content, NavigableString):\n",
        "            #content.string = content.string.replace('\\xa9', '(c)').replace('\\xa0', '')\n",
        "            if not content.string in [\"\", \"\\n\"]:\n",
        "                #print(content.string)\n",
        "                text_set.add(tag_string + content.string)\n",
        "        elif str.lower(content.name) in [\"script\", \"meta\", \"style\", \"header\", \"footer\"]:\n",
        "            pass\n",
        "        else:\n",
        "            #print(content.name)\n",
        "            soup2textset(content, text_set, tag_string + str.lower(content.name))\n",
        "    return text_set\n",
        "\n",
        "# text_setに一致する葉ノードを削除する\n",
        "def removetags(tag, remove_text_set, tag_string):\n",
        "    remove_lists = []\n",
        "    for content in tag.contents:\n",
        "        if isinstance(content, NavigableString):\n",
        "            if content.string in [\"\", \"\\n\"] or (tag_string+content.string) in remove_text_set:\n",
        "                remove_lists.append(content)\n",
        "        elif str.lower(content.name) in [\"script\", \"meta\", \"style\", \"header\", \"footer\"]:\n",
        "            remove_lists.append(content)\n",
        "        else:\n",
        "            removetags(content, remove_text_set, tag_string + str.lower(content.name))\n",
        "    for content in remove_lists:\n",
        "        content.extract()\n",
        "\n",
        "# Webページを取得して解析する\n",
        "# def html2text(htmls: list(str)) -> list(str):\n",
        "# 同じドメインのhtml(string)群を受け取って、タグを削除したstringのlistを返す\n",
        "def html2text(htmls: list[string]) -> list[string]:\n",
        "    # 文字列抽出\n",
        "    soups = [BeautifulSoup(html, \"html.parser\") for html in htmls]\n",
        "    #bodies = [soup.find(\"body\").text for soup in soups]\n",
        "\n",
        "    bodies = [soup.find(\"body\") for soup in soups]\n",
        "\n",
        "    # 重複の削除\n",
        "    if 2 <= len(bodies):\n",
        "        # 重複要素の列挙\n",
        "        remove_set = frozenset(soup2textset(bodies[0], set(), \"\") & soup2textset(bodies[1], set(), \"\"))\n",
        "        #print(remove_set)\n",
        "        # 重複要素の削除\n",
        "        for i in range(len(bodies)):\n",
        "            removetags(bodies[i], remove_set, \"\")\n",
        "\n",
        "    #soup => textの変換\n",
        "    for i in range(len(bodies)):\n",
        "        bodies[i] = bodies[i].text\n",
        "\n",
        "    # 空行削除\n",
        "    emptyline_regex = re.compile('\\n\\\\s*\\n')\n",
        "    for i in range(len(bodies)):\n",
        "        bodies[i] = emptyline_regex.sub('\\n', bodies[i])\n",
        "\n",
        "    return bodies\n",
        "\n",
        "def test():\n",
        "    #load_url = \"https://codezine.jp/article/detail/12230\"\n",
        "    #html = requests.get(load_url).content\n",
        "    html = open(\"test.html\", 'r', encoding=\"utf-8\").read()\n",
        "    html2 = open(\"test2.html\", 'r', encoding=\"utf-8\").read()\n",
        "    bodies = html2text([html, html2])\n",
        "\n",
        "    # print(bodies[0][:100])\n",
        "    # print(bodies[0][-100:])\n",
        "    # print(\"====================\")\n",
        "    # print(bodies[1][:100])\n",
        "    # print(bodies[1][-100:])\n",
        "    # print(\"====================\")\n",
        "    print(bodies[0].replace('\\xa9', '(c)').replace('\\xa0', ''))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    test()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLr0j3yRav1t",
        "outputId": "0d1098aa-0473-4d57-8681-acc3ffae008d"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # データ一覧\n",
        "  print(os.listdir(\"./data\"))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['techacademy_15571.html', 'sejuku_105909.html', 'sejuku_3766.html', 'techacademy_15508.html', 'techacademy_15636.html', 'sejuku_33294.html']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}