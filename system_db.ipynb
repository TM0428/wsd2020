{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "system_db.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TM0428/wsd2020/blob/main/system_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px3DkvM326sM"
      },
      "source": [
        "# 必要パッケージのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zx8eh2EjrKo",
        "outputId": "7cf7b249-36a1-4f57-bb5f-5dff03e209da"
      },
      "source": [
        "!pip install -U beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting beautifulsoup4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 7.1MB/s \n",
            "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
            "Installing collected packages: soupsieve, beautifulsoup4\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0XR_jTSqORx",
        "outputId": "9e498349-8007-4114-ecc8-23a6485240dc"
      },
      "source": [
        "!apt-get install mecab\n",
        "!apt-get install libmecab-dev\n",
        "!apt-get install mecab-ipadic-utf8\n",
        "!pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -u\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/f0/b57bfb29abd6b898d7137f4a276a338d2565f28a2098d60714388d119f3e/mecab_python3-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (487kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGJGlDcO305O",
        "outputId": "f920251d-4a81-49bc-de56-ae2ed56fdfe9"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23pRMcRfWQ5p",
        "outputId": "c06f75e3-f7e4-4af1-f709-9cc445f40b55"
      },
      "source": [
        "# get html\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def dirsName(name: str):\n",
        "    return f\"data/{name}/\"\n",
        "\n",
        "def fileName(name: str, id: str, extension: str=\"html\")->str:\n",
        "    return dirsName(name) + id.replace('/','_') + \".\" + extension #idの階層がローカルの階層にならないように\n",
        "\n",
        "def urlName(article_url: str, article_id: str)->str:\n",
        "    return article_url + article_id\n",
        "\n",
        "def main():\n",
        "    contents = [ # name, domain, article_urlは手動でせざるを得ないか、article_idを自動で取ってこられるようにしたい\n",
        "        {\n",
        "            \"name\": \"sejuku\",\n",
        "            \"domain\":\"https://www.sejuku.net/\",\n",
        "            \"article_url\":\"https://www.sejuku.net/blog/\",\n",
        "            \"article_id\": ['33294', '3766', '105909'] # 記事idは https://www.sejuku.net/blog/curriculums-python から適当にとってきた。実際は https://www.sejuku.net/blog/archive/page/* から適当に取ってくるといいかもしれない\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"techacademy\",\n",
        "            \"domain\":\"https://techacademy.jp/\",\n",
        "            \"article_url\":\"https://techacademy.jp/magazine/\",\n",
        "            \"article_id\": ['15508', '15636', '15571'] # 記事idは https://techacademy.jp/magazine/category/programming/python/page/44 \n",
        "        },\n",
        "        {\n",
        "            \"name\": \"note_nkmk\",\n",
        "            \"domain\":\"https://note.nkmk.me/\",\n",
        "            \"article_url\":\"https://note.nkmk.me/\",\n",
        "            \"article_id\": ['python-union-find', 'python-numpy-sin-con-tan', 'python-list-2d-sort'] # 記事idは https://note.nkmk.me/ の新着記事・更新記事から適当にとってきた\n",
        "        },\n",
        "        # https://docs.python.org/ はスクレイピングだめらしい\n",
        "        \n",
        "        #{\n",
        "        #    \"name\": \"python_docs\",\n",
        "        #    \"domain\":\"https://docs.python.org/\",\n",
        "        #    \"article_url\":\"https://docs.python.org/ja/3/library\",\n",
        "        #    \"article_id\": ['pathlib', 'random', 're'] # 記事idは https://docs.python.org/ja/3/library/ から適当にとってきた\n",
        "        #},\n",
        "        {\n",
        "            \"name\": \"qiita\",\n",
        "            \"domain\": \"https://qiita.com/\",\n",
        "            \"article_url\": \"https://qiita.com/\", # https://qiita.com/をarticle_urlにするかhttps://qiita.com/user/をarticle_urlにするかは意見が分かれそう\n",
        "            \"article_id\": [\"kuroitu/items/f18acf87269f4267e8c1\", \"paleo_engineer/items/fdbde62df4394ddcaf11\", # 記事idは https://qiita.com/kai_kou/items/180a91fd88dbbbd746f6 から適当に取ってきた\n",
        "                            \"UTOG/items/c77c75a6aa61ee93fc6d\"] # idの階層がファイル名の階層になると階層構造が複雑になるので変換する必要がある\n",
        "        }\n",
        "        \n",
        "    ]\n",
        "    res = {}\n",
        "    for content in contents:\n",
        "        print(content)\n",
        "        res[content['domain']] = []\n",
        "        for id in content[\"article_id\"]:\n",
        "            filename = fileName(content[\"name\"], id) # ローカル保存用のファイル名\n",
        "            print(filename)\n",
        "            file_path = Path(filename)\n",
        "            print(file_path.exists())\n",
        "            html_code:str\n",
        "            if not file_path.exists(): # 1回getしたらローカルに保存して、複数回getしない\n",
        "                urlname = urlName(content['article_url'], id) # get用のurl名\n",
        "                print(urlname)\n",
        "                html = requests.get(urlname) # getしたhtml(bytes)\n",
        "                html_code = html.content.decode('utf-8') #getしたhtml(string)\n",
        "                dir_path = Path(dirsName(content['name']))\n",
        "                if not dir_path.exists():\n",
        "                    dir_path.mkdir(mode=0o777,parents=True,exist_ok=False) # ディレクトリを作成\n",
        "                with open(filename, mode='w', encoding='utf-8') as f:\n",
        "                    f.write(html_code) # ローカルに保存\n",
        "                time.sleep(3) # 適度なインターバル\n",
        "            with open(filename, mode='r', encoding='utf-8') as f:\n",
        "                html_code = f.read()\n",
        "                res[content['domain']].append(html_code)\n",
        "            \n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    a = main()\n",
        "    print(len(a['https://www.sejuku.net/'][0]))\n",
        "    print(len(a['https://techacademy.jp/'][1]))\n",
        "    print(len(a['https://note.nkmk.me/'][2]))\n",
        "    print(len(a['https://qiita.com/'][0]))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'sejuku', 'domain': 'https://www.sejuku.net/', 'article_url': 'https://www.sejuku.net/blog/', 'article_id': ['33294', '3766', '105909']}\n",
            "data/sejuku/33294.html\n",
            "False\n",
            "https://www.sejuku.net/blog/33294\n",
            "data/sejuku/3766.html\n",
            "False\n",
            "https://www.sejuku.net/blog/3766\n",
            "data/sejuku/105909.html\n",
            "False\n",
            "https://www.sejuku.net/blog/105909\n",
            "{'name': 'techacademy', 'domain': 'https://techacademy.jp/', 'article_url': 'https://techacademy.jp/magazine/', 'article_id': ['15508', '15636', '15571']}\n",
            "data/techacademy/15508.html\n",
            "False\n",
            "https://techacademy.jp/magazine/15508\n",
            "data/techacademy/15636.html\n",
            "False\n",
            "https://techacademy.jp/magazine/15636\n",
            "data/techacademy/15571.html\n",
            "False\n",
            "https://techacademy.jp/magazine/15571\n",
            "{'name': 'note_nkmk', 'domain': 'https://note.nkmk.me/', 'article_url': 'https://note.nkmk.me/', 'article_id': ['python-union-find', 'python-numpy-sin-con-tan', 'python-list-2d-sort']}\n",
            "data/note_nkmk/python-union-find.html\n",
            "False\n",
            "https://note.nkmk.me/python-union-find\n",
            "data/note_nkmk/python-numpy-sin-con-tan.html\n",
            "False\n",
            "https://note.nkmk.me/python-numpy-sin-con-tan\n",
            "data/note_nkmk/python-list-2d-sort.html\n",
            "False\n",
            "https://note.nkmk.me/python-list-2d-sort\n",
            "{'name': 'qiita', 'domain': 'https://qiita.com/', 'article_url': 'https://qiita.com/', 'article_id': ['kuroitu/items/f18acf87269f4267e8c1', 'paleo_engineer/items/fdbde62df4394ddcaf11', 'UTOG/items/c77c75a6aa61ee93fc6d']}\n",
            "data/qiita/kuroitu_items_f18acf87269f4267e8c1.html\n",
            "False\n",
            "https://qiita.com/kuroitu/items/f18acf87269f4267e8c1\n",
            "data/qiita/paleo_engineer_items_fdbde62df4394ddcaf11.html\n",
            "False\n",
            "https://qiita.com/paleo_engineer/items/fdbde62df4394ddcaf11\n",
            "data/qiita/UTOG_items_c77c75a6aa61ee93fc6d.html\n",
            "False\n",
            "https://qiita.com/UTOG/items/c77c75a6aa61ee93fc6d\n",
            "137020\n",
            "61371\n",
            "43725\n",
            "286137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTnV42vWW-6I",
        "outputId": "ee0dc667-274c-4898-8b45-bc7da47cccfc"
      },
      "source": [
        "# html to text\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import NavigableString\n",
        "import re\n",
        "\n",
        "# text_setを生成する\n",
        "# text_setは各葉ノードのテキストと木構造を表す\n",
        "def soup2textset(tag, text_set, tag_string):\n",
        "    for content in tag.contents:\n",
        "        if isinstance(content, NavigableString):\n",
        "            #content.string = content.string.replace('\\xa9', '(c)').replace('\\xa0', '')\n",
        "            text_set.add(tag_string + content.string)\n",
        "        elif str.lower(content.name) in [\"script\", \"meta\", \"style\", \"header\", \"footer\"]:\n",
        "            pass\n",
        "        else:\n",
        "            #print(content.name)\n",
        "            soup2textset(content, text_set, tag_string + str.lower(content.name))\n",
        "    return text_set\n",
        "\n",
        "# text_setに一致する葉ノードを削除する\n",
        "def removetags(tag, remove_text_set, tag_string):\n",
        "    remove_lists = []\n",
        "    for content in tag.contents:\n",
        "        if isinstance(content, NavigableString):\n",
        "            if (tag_string+content.string) in remove_text_set:\n",
        "                remove_lists.append(content)\n",
        "        elif str.lower(content.name) in [\"script\", \"meta\", \"style\", \"header\", \"footer\"]:\n",
        "            remove_lists.append(content)\n",
        "        else:\n",
        "            removetags(content, remove_text_set, tag_string + str.lower(content.name))\n",
        "    for content in remove_lists:\n",
        "        content.extract()\n",
        "\n",
        "from typing import List\n",
        "# Webページを取得して解析する\n",
        "# def html2text(htmls: list(str)) -> list(str):\n",
        "# 同じドメインのhtml(string)群を受け取って、タグを削除したstringのlistを返す\n",
        "def html2text(htmls: List[str]) -> List[str]:\n",
        "    # 文字列抽出\n",
        "    soups = [BeautifulSoup(html.replace(\"</li>\", \"\\n</li>\"), \"html.parser\") for html in htmls]\n",
        "    #bodies = [soup.find(\"body\").text for soup in soups]\n",
        "\n",
        "    bodies = [soup.find(\"body\") for soup in soups]\n",
        "\n",
        "    # 重複の削除\n",
        "    if 2 <= len(bodies):\n",
        "        # 重複要素の列挙\n",
        "        set0 = soup2textset(bodies[0], set(), \"\")\n",
        "        set1 = soup2textset(bodies[1], set(), \"\")\n",
        "        remove_set = frozenset(set0 & set1)\n",
        "        if 3 <= len(bodies):\n",
        "            set2 = soup2textset(bodies[2], set(), \"\")\n",
        "            remove_set = frozenset(remove_set | (set2 & set0)  | (set2 & set1))\n",
        "        #print(remove_set)\n",
        "        # 重複要素の削除\n",
        "        for i in range(len(bodies)):\n",
        "            removetags(bodies[i], remove_set, \"\")\n",
        "\n",
        "    #soup => textの変換\n",
        "    for i in range(len(bodies)):\n",
        "        bodies[i] = bodies[i].text\n",
        "\n",
        "    # 空行削除\n",
        "    emptyline_regex = re.compile('\\n\\\\s*\\n')\n",
        "    for i in range(len(bodies)):\n",
        "        bodies[i] = emptyline_regex.sub('\\n', bodies[i])\n",
        "\n",
        "    return bodies\n",
        "\n",
        "def test():\n",
        "    import os\n",
        "    html_names = os.listdir(\"./data/sejuku\")\n",
        "    htmls = [open(\"./data/sejuku/\" + html, 'r', encoding=\"utf-8\").read() for html in html_names]\n",
        "    bodies = html2text(htmls)\n",
        "\n",
        "    print(html_names[0])\n",
        "    print(bodies[0][:100])\n",
        "    print(\"........\")\n",
        "    print(bodies[0][-100:])\n",
        "    print(\"====================\")\n",
        "    print(html_names[1])\n",
        "    print(bodies[1][:100])\n",
        "    print(\"........\")\n",
        "    print(bodies[1][-100:])\n",
        "    print(\"====================\")\n",
        "    print(html_names[2])\n",
        "    print(bodies[2][:100])\n",
        "    print(\"........\")\n",
        "    print(bodies[2][-100:])\n",
        "    print(\"====================\")\n",
        "    # print(bodies[0].replace('\\xa9', '(c)').replace('\\xa0', ''))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    test()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3766.html\n",
            "Cloud9を導入すれば、簡単にPythonの開発環境を構築できます！Python学習したい！そう思ってもプログラミングを始めるところにすらたどり着けないという方もいらっしゃるかと思います。今回はCl\n",
            "........\n",
            "選してまとめてみた！第1回「プログラミング能力検定」12/7-13、受検料無料プログラミングを独学で学習する方法と挫折をなくす3つのコツ初心者向けプログラミング言語おすすめランキング【2020年最新】\n",
            "====================\n",
            "33294.html\n",
            "\n",
            "Pythonを自分のPCにインストールしたいけどどうやったらいいの？\n",
            "Pythonのバージョンを確認するには？と疑問を持たれている方もいるのではないでしょうか？言語を本格的に学ぶためには環境を構築す\n",
            "........\n",
            "（ただし暫く何も足さない）がいいんじゃないかというメモ - min.t (ミント)未経験からフリーランスエンジニアとして仕事獲得するまでの全手順プログラミング初心者は何からすべき？習得までの7ステップ\n",
            "====================\n",
            "105909.html\n",
            "\n",
            "Pythonの動かし方がいまいちわからない……\n",
            "実行したらエラーが発生したけど、どうすればいいの？\n",
            "おすすめの実行環境があったら教えてほしいこれからPythonを学習しようとしている初心者の方にとっ\n",
            "........\n",
            "く何も足さない）がいいんじゃないかというメモ - min.t (ミント)\n",
            "【2020年度の合格者速報】名門私立小学校受験・幼稚園受験なら「AIペーパー分析」×「マンツーマン指導」のアイキューキッズ\n",
            " \n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRd8pHbk0gPl"
      },
      "source": [
        "#ダウンロードするための関数です。\n",
        "import urllib.request\n",
        "def download(url, title):\n",
        "    urllib.request.urlretrieve(url,\"{0}\".format(title))\n",
        "download('https://dumps.wikimedia.org/jawiki/latest/jawiki-latest-pages-articles.xml.bz2','jawiki-latest-pages-articles.xml.bz2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsxQdHdf4AMV"
      },
      "source": [
        "#コーパス作ります。\n",
        "import gensim\n",
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "from gensim.models.doc2vec import Doc2Vec, Doc2VecVocab, TaggedDocument\n",
        "wiki = WikiCorpus('jawiki-latest-pages-articles.xml.bz2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgXdm7eE4Ima"
      },
      "source": [
        "class TaggedWikiDocument(object):\n",
        "   def __init__(self, wiki):\n",
        "       self.wiki = wiki\n",
        "       self.wiki.metadata = True\n",
        "   def __iter__(self):\n",
        "       for content, (page_id, title) in self.wiki.get_texts():\n",
        "           yield TaggedDocument([c for c in content], [title])\n",
        "          \n",
        "document = TaggedWikiDocument(wiki)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtVWfd8n4RW0"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "model = Doc2Vec(documents=document, dm=1, vector_size=100, window=8, min_count=10, epochs=5, workers=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_KeIeD-_YYA",
        "outputId": "02d8f2e2-1ad5-4fab-a3e0-0ae4b6f7ea04"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwcY4qmckbMR"
      },
      "source": [
        "import MeCab\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import os\n",
        "\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_path=\"/content/drive/MyDrive/東工大/システム設計演習/jawiki.doc2vec.dbow300d/jawiki.doc2vec.dbow300d.model\"):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "\n",
        "        Args:\n",
        "            model_path (str, optional): Defaults to \"jawiki.doc2vec.dbow300d.model\".\n",
        "        \"\"\"\n",
        "        self.model = Doc2Vec.load(model_path)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"\n",
        "        分かち書きを行うメソッド\n",
        "\n",
        "        Args:\n",
        "            text (str): 入力文字列\n",
        "\n",
        "        Returns:\n",
        "            list[str]: 分かち書き済み単語リスト\n",
        "        \"\"\"\n",
        "        wakati = MeCab.Tagger(\"-Owakati\")\n",
        "        wakati.parse(\"\")\n",
        "        return wakati.parse(text).strip().split()\n",
        "\n",
        "    def doc2vec(self, text):\n",
        "        \"\"\"\n",
        "        doc2vec\n",
        "\n",
        "        Args:\n",
        "            text (str): string\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: embedded vector\n",
        "        \"\"\"\n",
        "        return self.model.infer_vector(self._tokenize(text))\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = 'はじめに今まで自分が使ってきた中で、これは生産性が爆上げする！と思うものを厳選しました是非最後までご覧ください'\n",
        "    embedder = Embedder()\n",
        "    print(embedder.doc2vec(text))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "  \"\"\"\n",
        "  # データ一覧\n",
        "  html_names = os.listdir(\"./data/sejuku\")\n",
        "  htmls = [open(\"./data/sejuku/\" + html, 'r', encoding=\"utf-8\").read() for html in html_names]\n",
        "  bodies = html2text(htmls)\n",
        "  embedder = Embedder()\n",
        "  for body in bodies:\n",
        "    print(embedder.doc2vec(body))\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}